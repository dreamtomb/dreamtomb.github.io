<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>DreamTomb - 好想30岁退休</title><meta name="author" content="魏涛"><meta name="copyright" content="魏涛"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1. 星河璀璨，我虽仰望，却也借这星光，低头沉默却坚定地走下去。 2. 一切焦虑都来源于想得太多而做的太少。">
<meta property="og:type" content="website">
<meta property="og:title" content="DreamTomb">
<meta property="og:url" content="https://dreamtomb.github.io/page/4/index.html">
<meta property="og:site_name" content="DreamTomb">
<meta property="og:description" content="1. 星河璀璨，我虽仰望，却也借这星光，低头沉默却坚定地走下去。 2. 一切焦虑都来源于想得太多而做的太少。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:author" content="魏涛">
<meta property="article:tag" content="dreamtomb">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://dreamtomb.github.io/page/4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'DreamTomb',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2024-03-01 17:52:25'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/index_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="DreamTomb"><span class="site-name">DreamTomb</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">DreamTomb</h1><div id="site-subtitle"><span id="subtitle"></span></div><div id="site_social_icons"><a class="social-icon" href="https://github.com/dreamtomb" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/24819.html" title="LLM 数据处理方法">LLM 数据处理方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-04-14T00:36:04.000Z" title="发表于 2023-04-14 08:36:04">2023-04-14</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span></div><div class="content">GPT-3数据处理方法使用数据集：Common Crawl、WebText2、Books1、Books2以及Wikipedia的混合。
处理方法：

首先基于同更高质量数据集WebText(reddit karmma&gt;3)的相似性对下载的Common Crawl数据集进行过滤。具体的操作是训练一个二分类的分类器，将WebText作为正样本，Common Crawl作为负样本，进行训练。然后用训练好的分类器重新对Common Crawl进行重采样（只要被预测为高质量的数据）。然后再用多种高质量数据集的并集作为正样本，未过滤的Common Crawl作为负样本进行训练，得到的分类器对Common Crawl进行打分，只保留$np.random.pareto(\alpha)&gt;1−document_score$的内容。

利用Spark中的MinHashLSH算法（集合之间的相似度）进行了文章级别的去重操作。

增加了一些新的高质量数据集。除了筛选过的Common Crawl，还增加了WebText2、Books1、Books2以及Wikipedia数据集。

各数据集的采样比例如 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/32612.html" title="A Survey of Large Language Models论文笔记">A Survey of Large Language Models论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-04-06T00:32:41.000Z" title="发表于 2023-04-06 08:32:41">2023-04-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span></div><div class="content">A Survey of Large Language Models论文笔记整篇综述的结构如下：

Introduction(2页)
Overview(1.5页)
Resources of LLMs(4页)
Publicly available model checkpoints or APIs
Commonly used corpora
Library resource


Pre-training(7.5页)
Data collection
Architecture
Model Training


Adaptation tuning of LLMs(5页)
Instruction tuning
Alignment tuning


Utilization(3.5页)
In-context learning
Chain-of-thought prompting


Capacity evaluation(7.5页)
Basic evaluation tasks
Advanced ability evaluation
Public benchmarks and empirical an ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/5555.html" title="Prefix-Tuning_Prompt-Tuning论文笔记">Prefix-Tuning_Prompt-Tuning论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-03-27T01:45:33.000Z" title="发表于 2023-03-27 09:45:33">2023-03-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span></div><div class="content">Prefix-Tuning_Prompt-Tuning论文笔记当前NLP任务的主流方向大致有两种：预训练模型+finetuning以及预训练模型+Prompt+预测。前者存在着种种问题：首先，预训练的训练形式与下游任务有很大的鸿沟，难以完全发挥预训练模型的潜能，而且需要大量数据去填补这样的鸿沟，直接导致这种方法在下游任务数据不足的时候学习能力差。其次，数千亿参数的预训练模型在fine-tuning的时候需要庞大的算力和显存以及很长的时间，成本太大，此外对于每一个形式的下游任务都需要fine-tuning一个新模型去进行部署，过于冗余浪费。因此，prompt learning开始获得关注，这里先对prompt learning方法做一个简单的综述。
prompt learningGPT-3提出了In-Context Learning，证明了在Zero-shot、Few-shot场景下，模型不需要学习任何额外参数，只要在推断的过程中加入一些提示，就能达到不错的效果；这说明预训练模型中存在大量甚至可以说充足的知识，预训练模型本身学会的知识让它具有小样本学习能力。但是前面提到过，使用fine- ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/43168.html" title="T5论文笔记">T5论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-03-20T02:01:04.000Z" title="发表于 2023-03-20 10:01:04">2023-03-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span></div><div class="content">T5论文笔记
T5模型：是一个端到端，text-to-text预训练模型，是个基于Transformer的Encoder-Decoder模型。



这项工作最重要的贡献是给整个NLP预训练模型领域提供了一个通用框架，将所有NLP任务都转化成Text-to-Text（文本到文本）任务，用同样的模型，同样的损失函数，同样的训练过程，同样的解码过程来完成所有NLP任务，正如论文里所说的“introducing a unified framework that converts every language problem into a text-to-text format”。

作者公布了T5的代码模型：https://github.com/google-research/text-to-text-transfer-transformer，和C4数据集：https://www.tensorflow.org/datasets/catalog/c4。


模型为了解决Text-to-Text问题，目前主要有Encoder-Decoder、Languagemodel和Prefix LM三类结构 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/12602.html" title="GPT-3论文笔记">GPT-3论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2023-03-13T07:39:03.000Z" title="发表于 2023-03-13 15:39:03">2023-03-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLM/">LLM</a></span></div><div class="content">GPT-3论文笔记
扩展语言模型可以大大提高few-shot的性能(GPT-2的效果差强人意)，有时可以跟进行过fine-tuning的sota方法媲美。

对所有子任务，GPT-3都没有经过任何的梯度更新或者微调，直接用预训练模型测试表现(有工作[Language models are unsupervised multitask learners]证明了可以用由zero-shot模型转变来的预训练语言模型执行标准NLP任务)。

fine-tuning指的是给许多任务相关的有标签数据进行训练，训练过程中会更新参数。few-shot指的是在推断时给K个有标签数据，但是并不对其进行权重参数的更新。one-shot就是将few-shot中的K设置为1。zero-shot则是K&#x3D;0。

下图中可以看到zero-shot、one-shot、few-shot三种方法的表现随着参数或者in-context learning中例子数量的增加而提高。


整个模型的训练过程如下图所示：



数据与处理Common Crawl是一个很大的公开数据集，但是包含太多脏数据，因此需要对其进行过滤 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/713.html" title="Concealed Object Detection">Concealed Object Detection</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-11-07T03:28:31.000Z" title="发表于 2022-11-07 11:28:31">2022-11-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/weekly-paper/">weekly_paper</a></span></div><div class="content">论文笔记《Concealed Object Detection》
这篇论文是2021TPAMI的论文，也是2020年CVPR的oral论文Camouflaged Object Detection的改进。本文首先回顾一下其前身SINet，以及作者的另一篇用于息肉分割的伪装目标检测论文PraNet，最后再介绍SINet-V2。

SINet。生物学研究表明，捕食者在狩猎时，首先会判断潜在猎物是否存在，即会搜索猎物；然后，捕食者可以识别目标动物并进行捕食。  基于这个事实，作者提出了SINet——它包括两个主要模块:搜索模块（ search module，SM）和识别模块（ identification module，IM），前者负责寻找伪装的物体，而后者则用来精确地检测它。SINet的网络结构如下所示。

实验早已证明浅层的低级特征保留空间细节，用于构建目标边界，而深层的高级特征保留语义信息，用于定位目标。 由于神经网络的这种固有特性，作者将提取的特征划分为低级（2层），中级（1层），高级（2层）; 并通过拼接，上采样和下采样操作将它们组合起来。
在SM模块中，在5层卷积的基础上，将[X0 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/59520.html" title="PraNet Parallel Reverse Attention Network for Polyp Segmentation">PraNet Parallel Reverse Attention Network for Polyp Segmentation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-10-31T02:53:36.000Z" title="发表于 2022-10-31 10:53:36">2022-10-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/weekly-paper/">weekly_paper</a></span></div><div class="content">论文笔记《PraNet Parallel Reverse Attention Network for Polyp Segmentation》
设计了并行反向注意力网络（parallel reverse attention network，PraNet）。该模型的效果相比UNet家族有很大的提升，提升程度如下图所示。


总的来看，目前看到的两个效果最好的PraNet和SINet-V2都是采用了粗糙定位再加精确分割的手段来进行伪装目标检测。PraNet的模型结构如下图所示。


设计了并行部分解码器（parallel partial decoder，PPD），该模块会聚集高维特征，然后基于组合特征生成一个全局图作为后续组件的初始引导。


设计了反向注意力模块（reverse attention，RA）来挖掘边界线索。作者没有聚合来自所有层次的特征，而是在三个并行的高级特征中自适应地学习反向注意力，不断地从高层输出特征中擦除前景对象的方式来逐步挖掘互补区域和细节。具体操作是将深层中输出的特征图上采样激活获得预测图，然后翻转，如第一幅图所示，再通本层特征图相乘，即可擦除目前已知的前景区域。 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/11086.html" title="2022-10-24论文笔记">2022-10-24论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-09-02T02:35:22.000Z" title="发表于 2022-09-02 10:35:22">2022-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/weekly-paper/">weekly_paper</a></span></div><div class="content">论文笔记《Deep Texture-Aware Features for Camouflaged Object Detection》
TANet通过构建多个纹理感知细化模块，学习深度卷积神经网络中的纹理感知特征，来放大伪装目标与背景之间的细微纹理差异，用于伪装目标检测。纹理感知细化模块计算特征响应的协方差矩阵以提取纹理信息，此外作者还设计一个亲和力损失学习一组参数，有助于分离伪装目标和背景之间的纹理，并采用边界一致性损失来探索目标的细节结构。



挪用了残差细化模块（residual refine blocks，RRB）对不同层次的特征图进行细化，增强细节，去除背景噪声。


设计了纹理感知细化模块（texture-aware refinement module，TARM）来放大伪装物体与背景之间的纹理差异，从而显著增强了伪装物体的识别能力。先用多个1$\times$1卷积获得多种特征图，这些特征图会在后面的操作中逐步学习纹理的不同方面。接着计算每个位置的不同通道间的协方差矩阵，通过该矩阵捕捉卷积特征之间的关联（如特征的组合、共现等），具体操作是将某个位置的C维向量和其转置相乘，由于 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/60232.html" title="2022-10-17论文笔记">2022-10-17论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-09-02T02:35:10.000Z" title="发表于 2022-09-02 10:35:10">2022-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/weekly-paper/">weekly_paper</a></span></div><div class="content">论文笔记《Zoom In and Out: A Mixed-scale Triplet Network for Camouflaged Object Detection》
提出了混合尺度的三联网络——ZoomNet，它模仿人类在观察模糊图像时的行为，即放大和缩小。具体而言，ZoomNet采用缩放策略，通过精心设计的尺度整合单元和层次化混合尺度单元学习具有判别力的混合尺度语义，充分挖掘候选对象与背景环境之间的细微线索。此外，考虑到难以区分的纹理带来的不确定性和模糊性，构造了一个简单而有效的正则化约束——不确定性感知loss，以促进模型在候选区域准确地产生更高置信度的预测。





《Learning Calibrated Medical Image Segmentation via Multi-rater Agreement Modeling》
在医学图像分割中往往会有多位评分者进行标注，如果只用其中一位的标注进行训练那么在其他标注上进行测试的效果就会比较差。以往的工作中通常采用的多数投票法或者首选评分法，但是这两种方法都忽略了多评估者注释中异同点中包含的的丰富信息。为了解决这个问题， ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/58810.html" title="2022-9-26论文笔记">2022-9-26论文笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2022-09-02T02:34:44.000Z" title="发表于 2022-09-02 10:34:44">2022-09-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/weekly-paper/">weekly_paper</a></span></div><div class="content">论文笔记《Cascade Graph Neural Networks for RGB-D Salient Object Detection》
提出了级联图神经网络（CAS-GNN），可以通过一组级联图全面地提取和推理这两个数据源之间的相互作用，以学习RGB-D SOD的强大表示。CAS-GNN分别处理这两个数据源，并采用一种新的级联图推理（CGR）模块来学习强大的密集特征嵌入，从中可以很容易地推断出显著性图。与以前的方法相比，对互补数据源之间的高级关系的明确建模和推理使我们能够更好地克服诸如遮挡和歧义之类的挑战。


CAS-GNN包含多个图，每个图都被用于处理一个特定级别的跨模态推理。每一个图都包含两种基础类型的节点：几何节点储存了深度特征，外观节点储存了RGB相关特征。每条边连接两种节点：同一模态不同尺度的节点或者不同模态但是同一尺度的节点。此外，为了增强对多级特征的推理能力，作者将前面的图合并为下面的级联图的两个域特定的引导节点。因此，整个模型共有三种类型的节点。

对于多尺度节点嵌入。使用金字塔池化模块PPM、一个卷积层和一个插值层来提取两个模态的多尺度特征，作为初始的节点表示 ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/#content-inner">10</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">魏涛</div><div class="author-info__description">1. 星河璀璨，我虽仰望，却也借这星光，低头沉默却坚定地走下去。 2. 一切焦虑都来源于想得太多而做的太少。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/dreamtomb" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">公告栏信息</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/63715.html" title="2023-12-19论文笔记">2023-12-19论文笔记</a><time datetime="2023-12-19T09:24:57.000Z" title="发表于 2023-12-19 17:24:57">2023-12-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/62963.html" title="2023-12-05论文笔记">2023-12-05论文笔记</a><time datetime="2023-12-04T12:58:41.000Z" title="发表于 2023-12-04 20:58:41">2023-12-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/17086.html" title="2023-11-21论文笔记">2023-11-21论文笔记</a><time datetime="2023-11-21T10:33:12.000Z" title="发表于 2023-11-21 18:33:12">2023-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/35052.html" title="2023-11-7论文笔记">2023-11-7论文笔记</a><time datetime="2023-11-07T10:09:42.000Z" title="发表于 2023-11-07 18:09:42">2023-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/20606.html" title="2023-10-10论文笔记">2023-10-10论文笔记</a><time datetime="2023-09-26T08:31:45.000Z" title="发表于 2023-09-26 16:31:45">2023-09-26</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Hexo-Theme-LiveMyLife/"><span class="card-category-list-name">Hexo-Theme-LiveMyLife</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLM/"><span class="card-category-list-name">LLM</span><span class="card-category-list-count">29</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Multi-Task-Learning/"><span class="card-category-list-name">Multi-Task Learning</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Path-Signature/"><span class="card-category-list-name">Path Signature</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Pooling-Semantic-Segmentation/"><span class="card-category-list-name">Pooling, Semantic Segmentation</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/RL/"><span class="card-category-list-name">RL</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Saliency-Object-Detection/"><span class="card-category-list-name">Saliency Object Detection</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Salient-Object-Detection/"><span class="card-category-list-name">Salient Object Detection</span><span class="card-category-list-count">10</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/image-segmentation/" style="font-size: 1.27em; color: #99a0a9">image segmentation</a> <a href="/tags/clash/" style="font-size: 1.1em; color: #999">clash</a> <a href="/tags/%E4%B8%80%E8%AF%AD%E6%83%8A%E9%86%92%E6%A2%A6%E4%B8%AD%E4%BA%BA/" style="font-size: 1.1em; color: #999">一语惊醒梦中人</a> <a href="/tags/Salient-Object-Detection-Camouflaged-Object-Detection/" style="font-size: 1.1em; color: #999">Salient Object Detection, Camouflaged Object Detection</a> <a href="/tags/hexo/" style="font-size: 1.1em; color: #999">hexo</a> <a href="/tags/Path-Signature/" style="font-size: 1.21em; color: #999ea4">Path Signature</a> <a href="/tags/salient-object-detection/" style="font-size: 1.5em; color: #99a9bf">salient object detection</a> <a href="/tags/medical-image-segmentation/" style="font-size: 1.1em; color: #999">medical image segmentation</a> <a href="/tags/Multi-Task-Learning/" style="font-size: 1.1em; color: #999">Multi-Task Learning</a> <a href="/tags/camouflaged-object-detection/" style="font-size: 1.1em; color: #999">camouflaged object detection</a> <a href="/tags/Salient-Object-Detection/" style="font-size: 1.39em; color: #99a4b4">Salient Object Detection</a> <a href="/tags/Saliency-Object-Detection/" style="font-size: 1.1em; color: #999">Saliency Object Detection</a> <a href="/tags/Hexo-Theme-LiveMyLife/" style="font-size: 1.1em; color: #999">Hexo-Theme-LiveMyLife</a> <a href="/tags/Pooling-Semantic-Segmentation/" style="font-size: 1.1em; color: #999">Pooling, Semantic Segmentation</a> <a href="/tags/LLM/" style="font-size: 1.44em; color: #99a7ba">LLM</a> <a href="/tags/object-detection/" style="font-size: 1.16em; color: #999b9e">object detection</a> <a href="/tags/weekly-paper/" style="font-size: 1.33em; color: #99a2af">weekly_paper</a> <a href="/tags/RL/" style="font-size: 1.21em; color: #999ea4">RL</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 1.1em; color: #999">读书笔记</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">十一月 2023</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">九月 2023</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/08/"><span class="card-archive-list-date">八月 2023</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/07/"><span class="card-archive-list-date">七月 2023</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/06/"><span class="card-archive-list-date">六月 2023</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/05/"><span class="card-archive-list-date">五月 2023</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/04/"><span class="card-archive-list-date">四月 2023</span><span class="card-archive-list-count">3</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">98</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">162.8k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2024-03-01T09:52:25.190Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 魏涛</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"><script>window.typedJSFn = {
  init: (str) => {
    window.typed = new Typed('#subtitle', Object.assign({
      strings: str,
      startDelay: 300,
      typeSpeed: 150,
      loop: true,
      backSpeed: 50,
    }, null))
  },
  run: (subtitleType) => {
    if (true) {
      if (typeof Typed === 'function') {
        subtitleType()
      } else {
        getScript('https://cdn.jsdelivr.net/npm/typed.js@2.1.0/dist/typed.umd.min.js').then(subtitleType)
      }
    } else {
      subtitleType()
    }
  }
}
</script><script>function subtitleType () {
  if (true) {
    typedJSFn.init(["你在抱怨什么呢","为明天到来的事，说人生像是没有意义","没有选择会是唯一的路","这不是你自己的问题，人终归要好好去生活"])
  } else {
    document.getElementById("subtitle").textContent = "你在抱怨什么呢"
  }
}
typedJSFn.run(subtitleType)</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>