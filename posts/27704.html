<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLaMa2 | DreamTomb</title><meta name="author" content="魏涛"><meta name="copyright" content="魏涛"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLAMA2：预训练+SFT+RLHF详解（官方）及对话指令SFT+LORA全流程记录（自己） LLAMA2 训练流程：  先经过自监督学习训练，得到llama2基座模型 再经过SFT再有标签的数据上进行有监督学习训练 再进行RLHF，其中使用拒绝采样和PPO算法 在RLHF流程中，使用到由用户偏好数据训练得到的两个RM，对RLHF的模型评判，反馈训练  Pretraining预训练阶段需要大量的">
<meta property="og:type" content="article">
<meta property="og:title" content="LLaMa2">
<meta property="og:url" content="https://dreamtomb.github.io/posts/27704.html">
<meta property="og:site_name" content="DreamTomb">
<meta property="og:description" content="LLAMA2：预训练+SFT+RLHF详解（官方）及对话指令SFT+LORA全流程记录（自己） LLAMA2 训练流程：  先经过自监督学习训练，得到llama2基座模型 再经过SFT再有标签的数据上进行有监督学习训练 再进行RLHF，其中使用拒绝采样和PPO算法 在RLHF流程中，使用到由用户偏好数据训练得到的两个RM，对RLHF的模型评判，反馈训练  Pretraining预训练阶段需要大量的">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-08-11T13:06:55.000Z">
<meta property="article:modified_time" content="2024-03-01T09:36:37.077Z">
<meta property="article:author" content="魏涛">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://dreamtomb.github.io/posts/27704.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLaMa2',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-01 17:36:37'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/default_top_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="DreamTomb"><span class="site-name">DreamTomb</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLaMa2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-11T13:06:55.000Z" title="发表于 2023-08-11 21:06:55">2023-08-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-01T09:36:37.077Z" title="更新于 2024-03-01 17:36:37">2024-03-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>10分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLaMa2"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="LLAMA2：预训练-SFT-RLHF详解（官方）及对话指令SFT-LORA全流程记录（自己）"><a href="#LLAMA2：预训练-SFT-RLHF详解（官方）及对话指令SFT-LORA全流程记录（自己）" class="headerlink" title="LLAMA2：预训练+SFT+RLHF详解（官方）及对话指令SFT+LORA全流程记录（自己）"></a>LLAMA2：预训练+SFT+RLHF详解（官方）及对话指令SFT+LORA全流程记录（自己）</h1><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/27704/LLAMA.png" alt="LLAMA.png"></p>
<p>LLAMA2 训练流程：</p>
<ul>
<li>先经过自监督学习训练，得到llama2基座模型</li>
<li>再经过SFT再有标签的数据上进行有监督学习训练</li>
<li>再进行RLHF，其中使用拒绝采样和PPO算法</li>
<li>在RLHF流程中，使用到由用户偏好数据训练得到的两个RM，对RLHF的模型评判，反馈训练</li>
</ul>
<h2 id="Pretraining"><a href="#Pretraining" class="headerlink" title="Pretraining"></a>Pretraining</h2><p>预训练阶段需要大量的数据，数据的量级往往是nT级别，通过预训练可以让模型学习到大量的知识。</p>
<h3 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h3><p>LLAMA2以LLAMA1中描述的预训练方法为基础，采用了optimized auto-regressive transformer。但是做了以下优化：1、进行了更鲁棒的数据清洗。2、更新了数据，使得总token数增加了40%（两万亿）。3、将上下文长度翻倍，并使用了grouped-query attention（GQA）来提高大模型推理的可扩展性。对比如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/27704/compare.png" alt="compare.png"></p>
<p>关于<font color="red">GQA</font>：在Transformer的decoder中每一个timestep都要将之前一个timestep的k,v保存到cache中，这就是KV Cache。这种方法对于小模型是没有问题的，但是遇到大模型的时候就行不通了——我们记transformer模型的层数为$l$，隐藏层维度为$h$，训练数据的批次大小为$b$，输入序列长度为$s$，输出序列长度为$n$，并且以fp16来保存KV Cache，那么<font color="red">KV cache的峰值显存占用大小为</font>$2\timesb\times(s+n)\timesh\timesl\times2$其中第一个2表示k、v，第二个2表示fp16占用的2个bytes，稍微带入计算就知道这个大小仿佛是在开玩笑，因此必须对MHA方法进行改进，大致有两种改法，如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/27704/GQA.png" alt="GQA.png"></p>
<p>首先是原始的 **MHA(Multi-Head Attention)**，QKV 三部分有相同数量的头，且一一对应。每次做 Attention，head1 的 QKV 就做好自己运算就可以，输出时各个头加起来就行。</p>
<p>而 MQA 则是，让 <strong>Q 仍然保持原来的头数</strong>，但 <strong>K 和 V 只有一个头</strong>，相当于所有的 Q 头共享一组 K 和 V 头，所以叫做 Multi-Query 了。实现改变了会不会影响效果呢？确实会影响但相对它能带来的收益，性能的些微降低是可以接受的。能带来多大的收益呢，实验发现一般能提高 30%-40% 的吞吐。收益主要就是由降低了 KV cache 带来的。实际上 MQA 运算量和 MHA 是差不多的，可理解为<strong>读取一组 KV 头</strong>之后，<strong>给所有 Q 头用</strong>，但因为之前提到的内存和计算的不对称，所以是有利的。</p>
<p>而 GQA 呢，是 MHA 和 MQA 的折衷方案，既不想损失性能太多，又想获得 MQA 带来的推理加速好处。具体思想是，不是所有 Q 头共享一组 KV，而是<strong>分组一定头数 Q 共享一组 KV</strong>，比如上面图片就是两组 Q 共享一组 KV。以上。</p>
<h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><table>
<thead>
<tr>
<th align="center">设置</th>
<th align="center">设置值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">架构</td>
<td align="center">standard transformer architecture (Vaswani et al., 2017)</td>
</tr>
<tr>
<td align="center">pre-normalization</td>
<td align="center">RMSNorm (Zhang and Sennrich, 2019)</td>
</tr>
<tr>
<td align="center">activation function</td>
<td align="center">SwiGLU (Shazeer, 2020)</td>
</tr>
<tr>
<td align="center">positional embedding</td>
<td align="center">rotary positional embeddings (RoPE, Su et al. 2022)</td>
</tr>
<tr>
<td align="center">optimizer</td>
<td align="center">AdamW (Loshchilov and Hutter, 2017)</td>
</tr>
<tr>
<td align="center">β1</td>
<td align="center">0.9</td>
</tr>
<tr>
<td align="center">β2</td>
<td align="center">0.95</td>
</tr>
<tr>
<td align="center">eps</td>
<td align="center">10 ^ −5</td>
</tr>
<tr>
<td align="center">schedule</td>
<td align="center">cosine learning rate schedule</td>
</tr>
<tr>
<td align="center">warmup</td>
<td align="center">2000 steps，decay final learning rate down to 10% of the peak learning rate</td>
</tr>
<tr>
<td align="center">weight decay</td>
<td align="center">0.1</td>
</tr>
<tr>
<td align="center">gradient clipping</td>
<td align="center">1.0</td>
</tr>
<tr>
<td align="center">tokenizer</td>
<td align="center">bytepair encoding (BPE，Sennrich et al., 2016) &amp;&amp; SentencePiece (Kudo and Richardson, 2018)</td>
</tr>
<tr>
<td align="center">vocabulary size</td>
<td align="center">32k tokens</td>
</tr>
</tbody></table>
<h3 id="训练结果评估"><a href="#训练结果评估" class="headerlink" title="训练结果评估"></a>训练结果评估</h3><p>作者对预训练模型进行了多种评估：</p>
<ul>
<li>代码：评估了我们的模型在HumanEval和MBPP上的平均pass@1得分；</li>
<li>常识推理：我们评估了PIQA、SIQA、HellaSwag、WinoGrande、ARC简单和挑战、OpenBookQA和CommonsenseQA的平均得分。对于CommonsenseQA，我们评估了7-shot结果，对于其他所有基准测试，我们评估了0-shot结果；</li>
<li>World Knowledge：我们评估了 NaturalQuestions 和 TriviaQA 的 5-shot 性能；</li>
<li>阅读理解：我们评估了在SQuAD、QuAC 和 BoolQ 上的0-shot平均值；</li>
<li>数学：我们评估了GSM8K（8个样本）和MATH（4个样本）基准测试的平均结果</li>
<li>Popular Aggregated Benchmarks：我们评估了MMLU（5个样本）、Big Bench Hard（3个样本）和AGI Eval（3-5个样本）的整体结果。对于AGI Eval，我们仅评估英语任务并报告平均结果。</li>
</ul>
<p>总的来说LLAMA2超过了所有的开源模型，但是在预训练阶段没有超过任何一个著名的闭源模型。</p>
<h2 id="Supervised-Fine-Tuning"><a href="#Supervised-Fine-Tuning" class="headerlink" title="Supervised Fine Tuning"></a>Supervised Fine Tuning</h2><p>在SFT阶段的最开始，使用的是《Scaling instruction-finetuned language models》这篇论文中贡献的指令微调数据集。</p>
<p>作者发现现有的指令微调数据集虽然很多，但是在多样性和数据质量上都难以令人满意，通过实验作者发现只需要10^4这个量级的高质量、高多样性的指令微调数据就足以获得高质量的结果了，因此作者只构建了27540条指令数据（自己的注释以及对其他数据集的筛选）。此外，标注人员的标注质量会很大程度地影响模型的性能。作者用微调过的模型生成的指令跟标注人员手写的指令进行了对比，发现两者的质量相当。</p>
<table>
<thead>
<tr>
<th align="center">设置</th>
<th align="center">设置值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">schedule</td>
<td align="center">cosine learning rate schedule</td>
</tr>
<tr>
<td align="center">initial learning rate</td>
<td align="center">2 x 10 ^ -5</td>
</tr>
<tr>
<td align="center">weight decay</td>
<td align="center">0.1</td>
</tr>
<tr>
<td align="center">batch size</td>
<td align="center">64</td>
</tr>
<tr>
<td align="center">sequence length</td>
<td align="center">4096 tokens</td>
</tr>
<tr>
<td align="center">sample</td>
<td align="center">由一个 prompt 和一个 answer 组成</td>
</tr>
<tr>
<td align="center">训练数据集</td>
<td align="center">所有prompt和answer都拼接起来，用一个特殊的token将两者区分开</td>
</tr>
<tr>
<td align="center">objective</td>
<td align="center">使用自回归目标，将来自prompt的token损失置零，只对answer token反向传播</td>
</tr>
<tr>
<td align="center">fine-tune epochs</td>
<td align="center">2</td>
</tr>
</tbody></table>
<p>LLAMA2分为基础版本和chat版本，这篇文章主要描述了基础版本，因此没有chat版本在对话数据上做SFT的描述。</p>
<h2 id="Reinforcement-Learning-with-Human-Feedback"><a href="#Reinforcement-Learning-with-Human-Feedback" class="headerlink" title="Reinforcement Learning with Human Feedback"></a>Reinforcement Learning with Human Feedback</h2><p>RLHF被应用在微调过的大语言模型上，目的是进一步让大语言模型的行为与<font color="red">人类偏好以及指令遵循</font>一致。作者首先收集了人类偏好数据（对两个模型的输出进行二元选择，挑出喜欢的那一个），然后利用人类反馈训练一个奖励模型，最后用奖励模型微调大语言模型。</p>
<h3 id="人类偏好数据收集"><a href="#人类偏好数据收集" class="headerlink" title="人类偏好数据收集"></a>人类偏好数据收集</h3><p>首先要求注释员编写一个prompt，然后根据提供的标准在两个模型响应中进行选择。为了最大化收集的prompt的多样性，给定prompt的两个响应是从两个不同的模型变体中采样的，并且改变了温度超参数。除了这种选择之外，还要求标注者对他们选择的回答和备选回答之间的偏好程度进行标注：明显更好，更好，稍微更好以及几乎没有区别。</p>
<h3 id="奖励模型建模"><a href="#奖励模型建模" class="headerlink" title="奖励模型建模"></a>奖励模型建模</h3><p>以一个prompt和它的响应作为输入，输出一个代表响应质量的标量。由于safety和helpfulness在有些时候时冲突的，因此作者训练了<font color="red">两个不同的奖励模型</font>，一个关注有用性，一个关注安全性。奖励模型是从预训练模型初始化来的，架构和超参数都完全一样，仅仅只将预测下一个token的分类头变成了预测质量评价标量的回归头。</p>
<p>作者将收集到的成对人类偏好数据转换为二元排名标签格式（即选择和拒绝），并确保所选响应的得分高于其余的响应。使用的损失函数是 binary ranking loss：<br>$$<br>\mathcal{L}_{ranking}&#x3D;-log(\sigma(r_\theta(x,y_c)-r_\theta(x,y_r)))<br>$$<br>其中$r_\theta(x,y)$代表使用模型权重$\theta$对提示x和响应y进行评分的标量分数。$y_c$是注释者选择的首选回复，而$y_r$是被拒绝的对应回复。这个评分用于训练奖励模型，以将人类偏好转化为二元排名标签格式，并确保首选回复的得分高于被拒绝的回复。此外，由于作者设置了四种偏好程度，可以利用这些信息来明确教导奖励模型为具有更大差异的生成结果分配更不一致的分数，因此对损失函数进行了修改：<br>$$<br>\mathcal{L}_{ranking}&#x3D;-log(\sigma(r_\theta(x,y_c)-r_\theta(x,y_r)-m(r)))<br>$$<br>$m(r)$是根据偏好评分来确定的离散函数。对于具有不同响应的对，使用较大的值，而对于具有相似响应的对，使用较小的值。</p>
<p>训练细节如下所示：</p>
<table>
<thead>
<tr>
<th align="center">设置</th>
<th align="center">设置值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">epoch</td>
<td align="center">1</td>
</tr>
<tr>
<td align="center">maximum learning rate</td>
<td align="center">5 x 10 ^ -6（70B）&amp;&amp; 1 x 10 ^ -5（others）</td>
</tr>
<tr>
<td align="center">schedule</td>
<td align="center">cosine learning rate schedule</td>
</tr>
<tr>
<td align="center">minimum learning rate</td>
<td align="center">10% of maximum</td>
</tr>
<tr>
<td align="center">warm up</td>
<td align="center">3% of total steps</td>
</tr>
<tr>
<td align="center">batch size</td>
<td align="center">512 pairs</td>
</tr>
</tbody></table>
<p>总的来说，meta的两个奖励模型要超过所有baseline模型，包括GPT-4.</p>
<h3 id="迭代化微调"><a href="#迭代化微调" class="headerlink" title="迭代化微调"></a>迭代化微调</h3><p>作者尝试了两种主要的RLHF微调算法：</p>
<ol>
<li>Proximal Policy Optimization（PPO）算法，由Schulman等人于2017年提出；</li>
<li>Rejection Sampling fine-tuning ：从模型中采样K个输出，使用奖励模型选择出最佳候选者，进一步使用选定的输出进行梯度更新。<font color="red">这种方法将获得最高奖励分数的样本视为新的标准，然后在这个新的排名样本集上进行模型的微调，加强奖励</font>。</li>
</ol>
<p>这两种 RL 的方法主要的区别在于：</p>
<ol>
<li><p>宽度：在拒绝抽样中，模型对于给定的提示会探索K个样本，而在PPO中，只生成一个样本；</p>
</li>
<li><p>深度：在PPO中，训练的每一步中，样本是基于上一步梯度更新后的更新模型策略的函数。而在拒绝抽样的微调中，我们会对初始模型策略生成的所有输出进行抽样，以收集新的数据集，然后进行类似于SFT的微调。然而，由于我们应用了迭代的模型更新，这两种强化学习算法之间的基本差异变得不那么明显。</p>
</li>
</ol>
<table>
<thead>
<tr>
<th align="center">设置</th>
<th align="center">设置值</th>
</tr>
</thead>
<tbody><tr>
<td align="center">optimizer</td>
<td align="center">AdamW</td>
</tr>
<tr>
<td align="center">β1</td>
<td align="center">0.9</td>
</tr>
<tr>
<td align="center">β2</td>
<td align="center">0.95</td>
</tr>
<tr>
<td align="center">eps</td>
<td align="center">10 ^ −5</td>
</tr>
<tr>
<td align="center">weight decay</td>
<td align="center">0.1</td>
</tr>
<tr>
<td align="center">gradient clipping</td>
<td align="center">1.0</td>
</tr>
<tr>
<td align="center">fixed learning rate</td>
<td align="center">10 ^ -6</td>
</tr>
<tr>
<td align="center">PPO batch size</td>
<td align="center">512</td>
</tr>
<tr>
<td align="center">PPO clip threshold</td>
<td align="center">0.2</td>
</tr>
<tr>
<td align="center">KL penalty</td>
<td align="center">0.01（7B，13B）&amp;&amp; 0.005（34B，70B）</td>
</tr>
<tr>
<td align="center">iterations</td>
<td align="center">200-400</td>
</tr>
</tbody></table>
<p>作者给出了一个重要的结论：<font color="red">SFT不能替代RLHF</font>（We posit that the superior writing abilities of LLMs, as manifested in surpassing human annotators in certain tasks, are fundamentally driven by RLHF, as documented in Gilardi et al. (2023) and Huang et al. (2023)）。</p>
<h2 id="System-Message-for-Multi-Turn-Consistency"><a href="#System-Message-for-Multi-Turn-Consistency" class="headerlink" title="System Message for Multi-Turn Consistency"></a>System Message for Multi-Turn Consistency</h2><p>作者详细描述了他们是如何通过Ghost Attention（GAtt）方法改进多轮对话一致性问题的。</p>
<p>在对话的场景下，我们通常需要设置system prompt，比如让他扮演xx角色，或者按照什么风格说话，作者发现几轮对话下来，模型容易忘记这样的“系统设定”。</p>
<p>假定有一个多轮对话数据集，用户和assistant之间交替对话，现将这个system prompt插入到每一轮对话中的用户消息中。然后用最新的RLHF模型从这个合成数据中选择样本，然后对于选择出来的样本，只在最开始保留system prompt，之前在每一轮对话中都插入了system prompt，现在要都移除掉，不然这个会跟之前模型RLHF以及SFT阶段不一致，为此，作者会把之前轮数的上下文的loss置为0（我的理解是依旧类似于SFT中把prompt的loss置为0）。最后，在训练过程中让模型根据这个新的样本数据进行微调，以便模型在后续的对话中能够继续遵循system prompt进行回答。</p>
<h2 id="safety"><a href="#safety" class="headerlink" title="safety"></a>safety</h2><p>略</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://dreamtomb.github.io">魏涛</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://dreamtomb.github.io/posts/27704.html">https://dreamtomb.github.io/posts/27704.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dreamtomb.github.io" target="_blank">DreamTomb</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/15520.html" title="Chinese_LLaMA2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Chinese_LLaMA2</div></div></a></div><div class="next-post pull-right"><a href="/posts/40640.html" title="LLM_inference_optimze"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">LLM_inference_optimze</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/20606.html" title="2023-10-10论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-26</div><div class="title">2023-10-10论文笔记</div></div></a></div><div><a href="/posts/17086.html" title="2023-11-21论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-21</div><div class="title">2023-11-21论文笔记</div></div></a></div><div><a href="/posts/35052.html" title="2023-11-7论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-07</div><div class="title">2023-11-7论文笔记</div></div></a></div><div><a href="/posts/62963.html" title="2023-12-05论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-04</div><div class="title">2023-12-05论文笔记</div></div></a></div><div><a href="/posts/63715.html" title="2023-12-19论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-19</div><div class="title">2023-12-19论文笔记</div></div></a></div><div><a href="/posts/32612.html" title="A Survey of Large Language Models论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-06</div><div class="title">A Survey of Large Language Models论文笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">魏涛</div><div class="author-info__description">1. 星河璀璨，我虽仰望，却也借这星光，低头沉默却坚定地走下去。 2. 一切焦虑都来源于想得太多而做的太少。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/dreamtomb" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">公告栏信息</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LLAMA2%EF%BC%9A%E9%A2%84%E8%AE%AD%E7%BB%83-SFT-RLHF%E8%AF%A6%E8%A7%A3%EF%BC%88%E5%AE%98%E6%96%B9%EF%BC%89%E5%8F%8A%E5%AF%B9%E8%AF%9D%E6%8C%87%E4%BB%A4SFT-LORA%E5%85%A8%E6%B5%81%E7%A8%8B%E8%AE%B0%E5%BD%95%EF%BC%88%E8%87%AA%E5%B7%B1%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">LLAMA2：预训练+SFT+RLHF详解（官方）及对话指令SFT+LORA全流程记录（自己）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pretraining"><span class="toc-number">1.1.</span> <span class="toc-text">Pretraining</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.1.</span> <span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82"><span class="toc-number">1.1.2.</span> <span class="toc-text">训练细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C%E8%AF%84%E4%BC%B0"><span class="toc-number">1.1.3.</span> <span class="toc-text">训练结果评估</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Supervised-Fine-Tuning"><span class="toc-number">1.2.</span> <span class="toc-text">Supervised Fine Tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reinforcement-Learning-with-Human-Feedback"><span class="toc-number">1.3.</span> <span class="toc-text">Reinforcement Learning with Human Feedback</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E7%B1%BB%E5%81%8F%E5%A5%BD%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86"><span class="toc-number">1.3.1.</span> <span class="toc-text">人类偏好数据收集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%96%E5%8A%B1%E6%A8%A1%E5%9E%8B%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.3.2.</span> <span class="toc-text">奖励模型建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%AD%E4%BB%A3%E5%8C%96%E5%BE%AE%E8%B0%83"><span class="toc-number">1.3.3.</span> <span class="toc-text">迭代化微调</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#System-Message-for-Multi-Turn-Consistency"><span class="toc-number">1.4.</span> <span class="toc-text">System Message for Multi-Turn Consistency</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#safety"><span class="toc-number">1.5.</span> <span class="toc-text">safety</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/63715.html" title="2023-12-19论文笔记">2023-12-19论文笔记</a><time datetime="2023-12-19T09:24:57.000Z" title="发表于 2023-12-19 17:24:57">2023-12-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/62963.html" title="2023-12-05论文笔记">2023-12-05论文笔记</a><time datetime="2023-12-04T12:58:41.000Z" title="发表于 2023-12-04 20:58:41">2023-12-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/17086.html" title="2023-11-21论文笔记">2023-11-21论文笔记</a><time datetime="2023-11-21T10:33:12.000Z" title="发表于 2023-11-21 18:33:12">2023-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/35052.html" title="2023-11-7论文笔记">2023-11-7论文笔记</a><time datetime="2023-11-07T10:09:42.000Z" title="发表于 2023-11-07 18:09:42">2023-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/20606.html" title="2023-10-10论文笔记">2023-10-10论文笔记</a><time datetime="2023-09-26T08:31:45.000Z" title="发表于 2023-09-26 16:31:45">2023-09-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 魏涛</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>