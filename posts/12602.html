<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>GPT-3论文笔记 | DreamTomb</title><meta name="author" content="魏涛"><meta name="copyright" content="魏涛"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="GPT-3论文笔记 扩展语言模型可以大大提高few-shot的性能(GPT-2的效果差强人意)，有时可以跟进行过fine-tuning的sota方法媲美。  对所有子任务，GPT-3都没有经过任何的梯度更新或者微调，直接用预训练模型测试表现(有工作[Language models are unsupervised multitask learners]证明了可以用由zero-shot模型转变来的预">
<meta property="og:type" content="article">
<meta property="og:title" content="GPT-3论文笔记">
<meta property="og:url" content="https://dreamtomb.github.io/posts/12602.html">
<meta property="og:site_name" content="DreamTomb">
<meta property="og:description" content="GPT-3论文笔记 扩展语言模型可以大大提高few-shot的性能(GPT-2的效果差强人意)，有时可以跟进行过fine-tuning的sota方法媲美。  对所有子任务，GPT-3都没有经过任何的梯度更新或者微调，直接用预训练模型测试表现(有工作[Language models are unsupervised multitask learners]证明了可以用由zero-shot模型转变来的预">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2023-03-13T07:39:03.000Z">
<meta property="article:modified_time" content="2024-03-01T09:36:37.057Z">
<meta property="article:author" content="魏涛">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://dreamtomb.github.io/posts/12602.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'GPT-3论文笔记',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-01 17:36:37'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.1.1"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/default_top_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="DreamTomb"><span class="site-name">DreamTomb</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">GPT-3论文笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-13T07:39:03.000Z" title="发表于 2023-03-13 15:39:03">2023-03-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-01T09:36:37.057Z" title="更新于 2024-03-01 17:36:37">2024-03-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLM/">LLM</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>7分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="GPT-3论文笔记"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="GPT-3论文笔记"><a href="#GPT-3论文笔记" class="headerlink" title="GPT-3论文笔记"></a>GPT-3论文笔记</h1><ol>
<li><p>扩展语言模型可以大大提高few-shot的性能(GPT-2的效果差强人意)，有时可以跟进行过fine-tuning的sota方法媲美。</p>
</li>
<li><p>对所有子任务，GPT-3都没有经过任何的梯度更新或者微调，直接用预训练模型测试表现(有工作[Language models are unsupervised multitask learners]证明了可以用由zero-shot模型转变来的预训练语言模型执行标准NLP任务)。</p>
</li>
<li><p>fine-tuning指的是给许多任务相关的有标签数据进行训练，训练过程中会更新参数。few-shot指的是在推断时给K个有标签数据，但是并不对其进行权重参数的更新。one-shot就是将few-shot中的K设置为1。zero-shot则是K&#x3D;0。</p>
</li>
<li><p>下图中可以看到zero-shot、one-shot、few-shot三种方法的表现随着参数或者in-context learning中例子数量的增加而提高。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/compare.png" alt="compare"></p>
</li>
<li><p>整个模型的训练过程如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/training_process.png" alt="training_process"></p>
</li>
</ol>
<h2 id="数据与处理"><a href="#数据与处理" class="headerlink" title="数据与处理"></a>数据与处理</h2><p>Common Crawl是一个很大的公开数据集，但是包含太多脏数据，因此需要对其进行过滤。具体的处理方式如下：</p>
<ol>
<li><p>首先基于同更高质量数据集WebText(reddit karmma&gt;3)的相似性对下载的Common Crawl数据集进行过滤。具体的操作是训练一个二分类的分类器，将WebText作为正样本，Common Crawl作为负样本，进行训练。然后用训练好的分类器重新对Common Crawl进行重采样（只要被预测为高质量的数据）。然后再用多种高质量数据集的并集作为正样本，未过滤的Common Crawl作为负样本进行训练，得到的分类器对Common Crawl进行打分，只保留$np.random.pareto(9)&gt;1-document_score$的内容。</p>
</li>
<li><p>利用Spark中的MinHashLSH算法（集合之间的相似度）进行了文章级别的去重操作。</p>
</li>
<li><p>增加了一些新的高质量数据集。除了筛选过的Common Crawl，还增加了WebText2、Books1、Books2以及Wikipedia数据集。</p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/dataset.png" alt="dataset"></p>
<h3 id="prompt-learning"><a href="#prompt-learning" class="headerlink" title="prompt learning"></a>prompt learning</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/prompt.png" alt="prompt"></p>
<p>Prompt learning是一种自然语言处理技术，旨在通过给定的prompt（提示语）来训练文本分类模型。在Prompt learning中，模型通过使用预定义的文本模板和提示来理解输入，并输出相应的文本分类标签。这些模板和提示可以是手动创建的，也可以通过自动化方法生成。</p>
<p>Prompt learning的优点是可以使模型更加可解释和透明，因为模型的输出可以直接映射到预定义的模板和提示上。此外，Prompt learning还可以在训练数据有限或标签不完整的情况下提高模型性能。</p>
<p>举例来说，对于情感分析任务，可以使用以下prompt：“这个产品怎么样？”作为输入，然后根据输出分类标签，例如“好评”或“差评”。通过这种方式，模型可以从输入中提取有关产品的信息，并根据其情感内容自动分类。</p>
<h2 id="架构及参数设置"><a href="#架构及参数设置" class="headerlink" title="架构及参数设置"></a>架构及参数设置</h2><p>模型的架构同GPT-2中的大都一样（modifified initialization,pre-normalization, and reversible tokenization），只有一点不同：作者在transformer的层中像[Sparse Transformer]中那样，交替使用密集和局部带状稀疏注意力模式。</p>
<p>设计了8个模型，分别进行了测试，所有模型的上下文窗口$n_{ctx}&#x3D;2048$个tokens。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/models.png" alt="models"></p>
<p>训练时使用Adam优化器，$\beta_1&#x3D;0.9$,$\beta_2&#x3D;0.95$,$\epsilon&#x3D;10^{-8}$，weight decay&#x3D;0.1, clip the global norm of the gradient at 1.0，使用Cross-Entropy loss，使用cosine decay调节学习率至原始值的10%（经过了2600亿个token之后训练继续从0.1lr进行）。在前3亿7500万个token采用linear LR warmup。实验中逐渐线性增大Batch Size，初始值为32k个token，经历40-120亿个（由模型的大小决定）token达到最大值。</p>
<h2 id="关于Sparse-Transformer"><a href="#关于Sparse-Transformer" class="headerlink" title="关于Sparse Transformer"></a>关于Sparse Transformer</h2><p>self-attention要对序列中任意两个向量计算相关度，因此计算复杂度是$O({n^2})$。所以，如果要节省显存，加快计算速度，那么一个基本的思路就是减少关联性的计算，也就是认为每个元素只跟序列内的一部分元素相关，这就是稀疏Attention的基本原理。OpenAI提出了两种Sparse Self Attention：Strided&#x2F;Fixed Self Attention，Full Self Attention和两种Sparse Self Attention分别如下图所示（该论文所做任务需要掩掉后续所有内容，只看到前面的内容，因此只保留了下三角矩阵，但是对于概念来讲，可以扩展到完整矩阵中）。如下图左一所示，Full Self Attention中进行$A_{ij}&#x3D;Q_iK_j^T$相乘时，针对的是所有的$j\le i$。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/Sparse_Self_Attention.png" alt="Sparse_Self_Attention"></p>
<h3 id="Atrous-Self-Attention"><a href="#Atrous-Self-Attention" class="headerlink" title="Atrous Self Attention"></a>Atrous Self Attention</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/Atrous_Self_Attention.png" alt="Atrous_Self_Attention"></p>
<p>如图所示，Atrous Self Attention就是启发于（Atrous Convolution），它对相关性进行了约束，强行要求每个元素只跟它相对距离为k,2k,3k,…的元素关联，其中k&gt;1是预先设定的超参数。从左边的注意力矩阵看，就是强行要求相对距离不是k的倍数的注意力为0（白色代表0）。具体来说，Atrous Self Attention就是进行$A_{ij}&#x3D;Q_iK_j^T$相乘时，针对的是所有的$\ j&#x3D;{(i-j)\  mod\  l&#x3D;0}$。通过使用这种自注意力，可以将运算复杂度变为$O(n^2&#x2F;k)$。</p>
<h3 id="Local-Self-Attention"><a href="#Local-Self-Attention" class="headerlink" title="Local Self Attention"></a>Local Self Attention</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/Local_Self_Attention.png" alt="Local_Self_Attention"></p>
<p>如图所示，Local Self Attention就是要放弃全局关联，使用局部关联,约束每个元素只与前后k个元素以及自身有关联，相对距离超过k的注意力都直接设为0。具体来说，Local Self Attention就是进行$A_{ij}&#x3D;Q_iK_j^T$相乘时，针对的是所有的$\ j&#x3D;{t,t+1,\cdots,i},\ t&#x3D;max(0,i-l)$。通过使用这种自注意力，可以将运算复杂度变为$O(kn)$，变为了线性关系这样的理想性质，不过要以牺牲长距离关联性为代价。</p>
<h3 id="Strided-Self-Attention"><a href="#Strided-Self-Attention" class="headerlink" title="Strided Self Attention"></a>Strided Self Attention</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/posts/12602/Strided_Self_Attention.png" alt="Strided_Self_Attention"></p>
<p>有上述两种attention作为前置知识，Strided Self Attention的提出便是顺理成章的了。Atrous Self Attention是带有一些洞的，而Local Self Attention正好填补了这些洞，所以一个简单的方式就是将Local Self Attention和Atrous Self Attention交替使用，两者结合起来，理论上既可以学习到全局关联性，又节省了显存和运算时间。而Strided Self Attention则是直接将二者合并为一个attention，如上图所示。</p>
<h3 id="Fixed-Self-Attention"><a href="#Fixed-Self-Attention" class="headerlink" title="Fixed Self Attention"></a>Fixed Self Attention</h3><p>作者指出，stride模式不适合用于文本数据上，因为文本不具备周期性结构(不像图像或者音乐)，因此又提出了类似的Fixed Self Attention。具体来说，就是在进行$A_{ij}&#x3D;Q_iK_j^T$相乘时，同样将注意力分成了两个部分，一个部分针对的是所有的$\ j&#x3D;{\ \lfloor j&#x2F;l\rfloor&#x3D;\lfloor i&#x2F;l\rfloor\ }$，另一个部分针对的是所有的$\ j&#x3D;{\ j\  mod\  l\in{t,t+1,\cdots,l-1}},\ t&#x3D;l-c$，其中$c$也是一个超参数，类似于滑动窗口的宽度，图中应当是c&#x3D;1的示例。针对后者，具体来说，如果设置l&#x3D;128，c&#x3D;8，那么所有大于128的像素都要关注120-128，所有大于256的像素都要关注248-256，引用原文中的表述即为：”specific cells summarize previous locations and propagate that information to all future cells”。</p>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>上面介绍了多种不同形式的注意力核，那么下面我们将介绍如何将这些不同形式的注意力核融入到网络中的，在论文中作者介绍了三种融合的方式：</p>
<ol>
<li><p>每个残差块使用不同的注意力核。对于一个深度网络，它是由连续的残差块组成的，对于每个残差块，我们可以使用不同类型的注意力核。公式如下：<br>$$<br>attention(X)&#x3D;W_p\cdot attend(X,A^{(r\ mod\ p)})<br>$$<br>其中，r是残差块的索引，p是分解注意力头的数目。</p>
</li>
<li><p>每个注意力头都计算所有类型的注意力核，然后合并他们的结果。<br>$$<br>attention(X)&#x3D;W_p\cdot attend(X,\bigcup_{m&#x3D;1}^p A^{(m)})<br>$$</p>
</li>
<li><p>使用多头注意力机制，每组头选择一个形式的注意力核，然后将它们合并起来(特征维度拼接)。实验表明这种方式是最好的融合策略。<br>$$<br>attention(X)&#x3D;W_p\cdot\left(attend(X,A)^{(i)}\right)_{i\in{1,\cdots,n_h}}<br>$$</p>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://dreamtomb.github.io">魏涛</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://dreamtomb.github.io/posts/12602.html">https://dreamtomb.github.io/posts/12602.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://dreamtomb.github.io" target="_blank">DreamTomb</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/43168.html" title="T5论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">T5论文笔记</div></div></a></div><div class="next-post pull-right"><a href="/posts/713.html" title="Concealed Object Detection"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Concealed Object Detection</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/20606.html" title="2023-10-10论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-26</div><div class="title">2023-10-10论文笔记</div></div></a></div><div><a href="/posts/17086.html" title="2023-11-21论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-21</div><div class="title">2023-11-21论文笔记</div></div></a></div><div><a href="/posts/35052.html" title="2023-11-7论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-11-07</div><div class="title">2023-11-7论文笔记</div></div></a></div><div><a href="/posts/62963.html" title="2023-12-05论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-04</div><div class="title">2023-12-05论文笔记</div></div></a></div><div><a href="/posts/63715.html" title="2023-12-19论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-19</div><div class="title">2023-12-19论文笔记</div></div></a></div><div><a href="/posts/32612.html" title="A Survey of Large Language Models论文笔记"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-06</div><div class="title">A Survey of Large Language Models论文笔记</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">魏涛</div><div class="author-info__description">1. 星河璀璨，我虽仰望，却也借这星光，低头沉默却坚定地走下去。 2. 一切焦虑都来源于想得太多而做的太少。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">98</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">19</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/dreamtomb" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">公告栏信息</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#GPT-3%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">GPT-3论文笔记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%8E%E5%A4%84%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">数据与处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#prompt-learning"><span class="toc-number">1.1.1.</span> <span class="toc-text">prompt learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="toc-number">1.2.</span> <span class="toc-text">架构及参数设置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8ESparse-Transformer"><span class="toc-number">1.3.</span> <span class="toc-text">关于Sparse Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Atrous-Self-Attention"><span class="toc-number">1.3.1.</span> <span class="toc-text">Atrous Self Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Local-Self-Attention"><span class="toc-number">1.3.2.</span> <span class="toc-text">Local Self Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Strided-Self-Attention"><span class="toc-number">1.3.3.</span> <span class="toc-text">Strided Self Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Fixed-Self-Attention"><span class="toc-number">1.3.4.</span> <span class="toc-text">Fixed Self Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Architecture"><span class="toc-number">1.3.5.</span> <span class="toc-text">Architecture</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/63715.html" title="2023-12-19论文笔记">2023-12-19论文笔记</a><time datetime="2023-12-19T09:24:57.000Z" title="发表于 2023-12-19 17:24:57">2023-12-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/62963.html" title="2023-12-05论文笔记">2023-12-05论文笔记</a><time datetime="2023-12-04T12:58:41.000Z" title="发表于 2023-12-04 20:58:41">2023-12-04</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/17086.html" title="2023-11-21论文笔记">2023-11-21论文笔记</a><time datetime="2023-11-21T10:33:12.000Z" title="发表于 2023-11-21 18:33:12">2023-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/35052.html" title="2023-11-7论文笔记">2023-11-7论文笔记</a><time datetime="2023-11-07T10:09:42.000Z" title="发表于 2023-11-07 18:09:42">2023-11-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/20606.html" title="2023-10-10论文笔记">2023-10-10论文笔记</a><time datetime="2023-09-26T08:31:45.000Z" title="发表于 2023-09-26 16:31:45">2023-09-26</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 魏涛</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.8.8/dist/lazyload.iife.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>